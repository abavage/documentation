# ps
ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem

# simple expansions
touch /tmp/{jjj,jj}

# deploy
oc create deploy tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest -- sleep infinity
oc create deploy tools --image=registry.access.redhat.com/ubi9/toolbox:latest -- sleep infinity
oc set resources deployment tools --limits=cpu=200m,memory=200Mi --requests=cpu=100m,memory=100Mi
oc set volume deploy/prometheus-one -t pvc --add --name=prometheus-one --claim-name=prometheus-one  -m /prometheus


oc run tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest -- sleep infinity
oc run tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest  --dry-run=client -o yaml

## Aduit logs
oc adm node-logs --role=master --path=kube-apiserver/
oc adm node-logs  <master_node> --path=kube-apiserver/audit-2023-11-09T05-32-38.217.log > audit-2023-11-09T05-32-38.217.log
 
while read -r line; do echo "$line" | jq -r '.objectRef.resource + "," + .verb + "," + .objectRef.namespace + "," + .objectRef.name + "," + .user.username' ; done < audit-2023-11-09T04-21-29.518.log >> sorted-audit-2023-11-09T04-21-29.518.log
 
grep "pods,watch" sorted-audit-2023-11-09T04-21-29.518.log | wc
grep "pods,watch" sorted-audit-2023-11-09T04-21-29.518.log | grep collect | wc

# thanos endpoint query
TOKEN=$(oc -n openshift-monitoring get secret $(oc -n openshift-monitoring get sa prometheus-k8s -o json  | jq -r '.secrets[].name') -o json | jq -r '.metadata.annotations."openshift.io/token-secret.value"')
curl -k -H "Authorization: Bearer $TOKEN" https://thanos-querier-openshift-monitoring.apps..../api/v1/query? --data-urlencode "query=up"

# thanos endpoitn list all metrics
TOKEN=$(oc -n openshift-monitoring get secret $(oc -n openshift-monitoring get sa prometheus-k8s -o json  | jq -r '.secrets[].name') -o json | jq -r '.metadata.annotations."openshift.io/token-secret.value"')
curl -k -H "Authorization: Bearer $TOKEN" https://thanos-querier-openshift-monitoring.apps..../api/v1/label/__name__/values | jq

# another variation
curl -s -g -k -X GET -H "Authorization: Bearer $TOKEN" -H 'Accept: application/json' -H 'Content-Type: application/json' "$URL/api/v1/alerts" | jq  -c '.data.alerts[] | select( .state == "firing" and .labels.severity == "critical")'

# KUBECONFIG
# On master nodes
ls -al /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/

export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig

* lb-ext.kubeconfig: It points to external API load balancer (api.<cluster-domain>)
* lb-int.kubeconfig: It points to internal API load balancer (api-int.<cluster-domain>)
* localhost.kubeconfig: It points to localhost. This one is useful in case of problems with the load balancers, for example.
* localhost-recovery.kubeconfig: It points to localhost but sends the localhost-recovery SNI name, which in turns causes the special server certificates used by the auto-recovery logic to be used. This one may be useful if kube-apiserver serving certificates are expired and the auto-recovery logic has failed.


# Prune object from etcd - see above for oc commands if needed
oc debug node/<master>

crictl ps | grep etcd
crictl exec -ti <some id> /bin/bash

# in the crictl container
etcdctl get "" --prefix --keys-only

# get desired item(s)
etcdctl get "" --prefix --keys-only | grep nonprod

# displays the item (secret) as plain text
etcdctl get /kubernetes.io/secrets/nonprod/file

# remove the secret 
etcdctl del /kubernetes.io/secrets/nonprod/file

# remove events from namespace
# events can consume considerable size also
for i in $(etcdctl get "" --prefix --keys-only | grep nonprod | grep events) ; do etcdctl del $i ; done


# get a count of etcd objects - kube standard
etcdctl get / --prefix --keys-only | sed '/^$/d' | cut -d/ -f3 | sort | uniq -c | sort -rn

# get a count of deploymentConfigs
etcdctl --command-timeout=60s get --prefix --keys-only /kubernetes.io/controllers | awk -F/ '/./ { print $3 }' | uniq -c

## Stop the kube-apiserver on a master
oc debug node/<node>
chroot /host

# cd into the static directory
cd /etc/kubernetes/static-pod-resources

# find the newest / largest number revision of the kube-apiserver-pod
ls -ltrd kube-apiserver-pod

# move the directory to tmp
mv kube-apiserver-pod-11 /tmp/

# stop all the kube-apiserver containers - should be 5 of them
# not the kube-apiserver-operator
crictl ps | grep kube-apiserver
crictl stop -t 5 <id> <id> <id> <id> <id>

# test it locally - should fail
export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig
oc get pods -n openshift-kube-apiserver

# start it back up 
mv /tmp/kube-apiserver-pod-11 /etc/kubernetes/static-pod-resources/

# Either wait for the CrashLoopBackOff timeout to expire or delete the apiserver pod from other shell
oc delete pod kube-apiserver-control-plane-cluster-vqmb2-1 -n openshift-kube-apiserver

# test it locally - should work
export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig
oc get pods -n openshift-kube-apiserver

# force redeployment - takes the operator 5 - 10mins to rollout
oc patch kubeapiserver/cluster --type merge -p "{\"spec\":{\"forceRedeploymentReason\":\"Forcing new revision with random number $(date --rfc-3339=ns) to make message unique\"}}"

# infrastructure
oc get infrastructure cluster  -o yaml

# patch ingress controller for logging
oc patch ingresscontrollers default --type='merge' -p '{"spec": {"logging": {"access": {"destination": {"type": "Container"}}}}}'

# patch kube-apiserver
oc patch apiserver cluster --type='merge' -p '{"spec": {"audit": {"profile": "AllRequestBodies"}}}'
oc get co authentication kube-apiserver openshift-apiserver
oc get pods -n openshift-kube-apiserver

## while read
podman images -n | awk '{print $1,$2}' | while read IMAGE TAG ; do echo -e $IMAGE:$TAG "---" '\c' ; podman inspect $IMAGE:$TAG | jq '.[].Created'  ; done


###
# Set the variable
$ CUSTOMER_DOMAIN="<cluster_name>.<URI>"

# Check connection through the LB
$ curl https://api-int.${CUSTOMER_DOMAIN}:6443/healthz -k -w "\n ##############\n %{url_effective} (%{remote_ip})\n ##############\n code: %{http_code}\n --- \n namelookup: %{time_namelookup}\n connect: %{time_connect}\n appconnect: %{time_appconnect}\n pretransfer: %{time_pretransfer}\n redirect: %{time_redirect}\n starttrans: %{time_starttransfer}\n ---\n total: %{time_total}\n\n"

# Check the connection through one of the Master node:
$ curl https://api-int.${CUSTOMER_DOMAIN}:6443/healthz -k -w "\n ##############\n %{url_effective} (%{remote_ip})\n ##############\n code: %{http_code}\n --- \n namelookup: %{time_namelookup}\n connect: %{time_connect}\n appconnect: %{time_appconnect}\n pretransfer: %{time_pretransfer}\n redirect: %{time_redirect}\n starttrans: %{time_starttransfer}\n ---\n total: %{time_total}\n\n" --resolve api-int.${CUSTOMER_DOMAIN}:6443:10.150.38.233
###

# core dumps
/var/lib/systemd/coredump/ 
coredumpctl list

# journalctl 
journalctl --no-pager --all --full --since today > /tmp/journal_today.out 2>&1

# login to the local image registry
oc login -u
oc debug node/<node> -n openshift-monitoring
podman login -u <username> -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000

# looking in the directory structure of the image registry
oc rsh image-registry-67fb6757bc-mnw2k
sh-4.4$ cd /registry/docker/registry/v2/repositories/
sh-4.4$ ls -ltr

## crc
eval $(crc oc-env)
crc start
crc stop

## curl send 4 x requests
curl -k https://some_url/?[1-4]

# fancy node display
oc get nodes | sort -k3 | sed '1h;1d;$!H;$!d;G'

# curl via a proxy
curl --proxy https://somename:port https://registry.connect.redhat.com

# delete events from namespaces
# need to test it
# do it with a for loop
oc delete events --all -n <namespace>

# jq with periods
oc get secrets ab-ecr-secret-three -o json | jq -r '.data.".dockerconfigjson"' | base64 -d

# query thanos
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/query --data-urlencode 'query=up' | jq

# get the data out of thanos
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/label/__name__/values | jq

# get the configured servicemonitors
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/targets | jq '.data.activeTargets[].scrapePool'

# ssh-ketgen
ssh-keygen -t rsa -N "" -f .ssh/one_id_rsa

# rm using inode
ls -i

find . -maxdepth 1 -type d -inum 1119276 -exec rm -r {} \;
find . -type d -inum 1119276 -exec rm -r {} \;

## .bashrc
if command -v oc &> /dev/null; then
  source <(oc completion bash)
fi

# create route edge
oc create deploy httpd --image=registry.redhat.io/ubi9/httpd-24:latest
oc cerate svc clusterip httpd --tcp=8080:8080
oc create route edge httpd --insecure-policy=Redirect --service=httpd

## git merge up main
# stay on the branch
git checkout feature-branch
git branch
git fetch origin
git merge origin/main
git push


# 
oc get --raw /api/v1/nodes/ip-10-0-11-191.ap-southeast-2.compute.internal/proxy/stats/summary | jq

# metrics on kubelet
curl -kv -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" https://x.x.x.x:10250/metrics/resource

## not tested -- The following command can be used to retrive a list of clusterrolebindings assigned to a given user:
oc get clusterrolebindings -o json | jq --arg USER $(oc whoami) '.items[] | select(.subjects[0].name==$USER)' |jq '.subjects[0].name,.roleRef.name'



# curl options
for i in {1..50}; do curl -X GET "http://localhost:8080" -o /dev/null -s -w "HTTP Status: %{http_code} | DNS: %{time_namelookup}s | Connect: %{time_connect}s | TLS: %{time_appconnect}s | StartTransfer: %{time_starttransfer}s | Total: %{time_total}s\n"  -H "Accept: application/json"; done

# rosa verify command
rosa verify network --subnet-ids <subnetid1>,<subnetid2>,<subnetid3>  --watch --region <region> --role-arn arn:aws:iam::xxxx:role/yyyy-Installer-Role
rosa verify quota
rosa list instance-types
rosa describe cluster -c this-cluster -o json | jq '.version.available_upgrades'
rosa list upgrade

# show which rosa_tf_version
rosa describe cluster -c this-cluster -o json | jq '.properties'

# add line after every comma
oc get node ip-10-0-12-157.ap-southeast-2.compute.internal --show-labels  | sed 's/,/,\'$'\n/g'

# cluster autoscaler status
oc get cm cluster-autoscaler-status -n kube-system

# helm
helm status gitops-operator -n openshift -o yaml

# read tls.key
openssl rsa -in tls.key -text -noout

# velero
velero get backup -n openshift-adp
velero backup logs schedule-1-20250821040055 -n openshift-adp
velero delete backup schedule-1-20250821034555 -n openshift-adp
velero schedule get -n openshift-adp
velero schedule pause schedule-1  -n openshift-adp

# label on a resource to stop backups
velero.io/exclude-from-backup: "true"

