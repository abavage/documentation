# ps
ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem

# simple expansions
touch /tmp/{jjj,jj}

# cuase I can never remember - stdout and stderr
ls -la > /dev/null 2>&1 

# stderr
ls -la 2> /dev/null

# add this to a shell script
exec > /tmp/somefile.log 2>&1

# basic stuff
find / -perm -u=s -type f 2>/dev/null
find / -perm -o=w -type f 2>/dev/null

# du -sh
du -sh -- .[!.]* * 2>/dev/null | sort -hr | head -n 10
du -sh /var/lib/containers/storage/overlay/* 2>/dev/null | sort -rh  | head -n 40

# deploy
oc create deploy tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest -- sleep infinity
oc create deploy tools --image=registry.access.redhat.com/ubi9/toolbox:latest -- sleep infinity
oc set resources deployment tools --limits=cpu=200m,memory=200Mi --requests=cpu=100m,memory=100Mi
oc set volume deploy/prometheus-one -t pvc --add --name=prometheus-one --claim-name=prometheus-one  -m /prometheus

# git conform files are set right in git
git ls-files --stage somefile.sh
git update-index --chmod=+x somefile.sh


oc run tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest -- sleep infinity
oc run tools --image=registry.redhat.io/openshift4/network-tools-rhel8:latest  --dry-run=client -o yaml

# scc
oc adm policy who-can use scc anyuid



## Aduit logs
oc adm node-logs --role=master --path=kube-apiserver/
oc adm node-logs  <master_node> --path=kube-apiserver/audit-2023-11-09T05-32-38.217.log > audit-2023-11-09T05-32-38.217.log
 
while read -r line; do echo "$line" | jq -r '.objectRef.resource + "," + .verb + "," + .objectRef.namespace + "," + .objectRef.name + "," + .user.username' ; done < audit-2023-11-09T04-21-29.518.log >> sorted-audit-2023-11-09T04-21-29.518.log
 
grep "pods,watch" sorted-audit-2023-11-09T04-21-29.518.log | wc
grep "pods,watch" sorted-audit-2023-11-09T04-21-29.518.log | grep collect | wc

# thanos endpoint query
TOKEN=$(oc -n openshift-monitoring get secret $(oc -n openshift-monitoring get sa prometheus-k8s -o json  | jq -r '.secrets[].name') -o json | jq -r '.metadata.annotations."openshift.io/token-secret.value"')
curl -k -H "Authorization: Bearer $TOKEN" https://thanos-querier-openshift-monitoring.apps..../api/v1/query? --data-urlencode "query=up"

# thanos endpoitn list all metrics
TOKEN=$(oc -n openshift-monitoring get secret $(oc -n openshift-monitoring get sa prometheus-k8s -o json  | jq -r '.secrets[].name') -o json | jq -r '.metadata.annotations."openshift.io/token-secret.value"')
curl -k -H "Authorization: Bearer $TOKEN" https://thanos-querier-openshift-monitoring.apps..../api/v1/label/__name__/values | jq

# another variation
curl -s -g -k -X GET -H "Authorization: Bearer $TOKEN" -H 'Accept: application/json' -H 'Content-Type: application/json' "$URL/api/v1/alerts" | jq  -c '.data.alerts[] | select( .state == "firing" and .labels.severity == "critical")'

# can-i
oc auth can-i get deployment --as=system:serviceaccount:prometheus:prometheus

# KUBECONFIG
# On master nodes
ls -al /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/

export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig

* lb-ext.kubeconfig: It points to external API load balancer (api.<cluster-domain>)
* lb-int.kubeconfig: It points to internal API load balancer (api-int.<cluster-domain>)
* localhost.kubeconfig: It points to localhost. This one is useful in case of problems with the load balancers, for example.
* localhost-recovery.kubeconfig: It points to localhost but sends the localhost-recovery SNI name, which in turns causes the special server certificates used by the auto-recovery logic to be used. This one may be useful if kube-apiserver serving certificates are expired and the auto-recovery logic has failed.


# Prune object from etcd - see above for oc commands if needed
oc debug node/<master>

crictl ps | grep etcd
crictl exec -ti <some id> /bin/bash

# in the crictl container
etcdctl get "" --prefix --keys-only

# get desired item(s)
etcdctl get "" --prefix --keys-only | grep nonprod

# displays the item (secret) as plain text
etcdctl get /kubernetes.io/secrets/nonprod/file

# remove the secret 
etcdctl del /kubernetes.io/secrets/nonprod/file

# remove events from namespace
# events can consume considerable size also
for i in $(etcdctl get "" --prefix --keys-only | grep nonprod | grep events) ; do etcdctl del $i ; done


# get a count of etcd objects - kube standard
etcdctl get / --prefix --keys-only | sed '/^$/d' | cut -d/ -f3 | sort | uniq -c | sort -rn

# get a count of deploymentConfigs
etcdctl --command-timeout=60s get --prefix --keys-only /kubernetes.io/controllers | awk -F/ '/./ { print $3 }' | uniq -c

## Stop the kube-apiserver on a master
oc debug node/<node>
chroot /host

# cd into the static directory
cd /etc/kubernetes/static-pod-resources

# find the newest / largest number revision of the kube-apiserver-pod
ls -ltrd kube-apiserver-pod

# move the directory to tmp
mv kube-apiserver-pod-11 /tmp/

# stop all the kube-apiserver containers - should be 5 of them
# not the kube-apiserver-operator
crictl ps | grep kube-apiserver
crictl stop -t 5 <id> <id> <id> <id> <id>

# test it locally - should fail
export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig
oc get pods -n openshift-kube-apiserver

# start it back up 
mv /tmp/kube-apiserver-pod-11 /etc/kubernetes/static-pod-resources/

# Either wait for the CrashLoopBackOff timeout to expire or delete the apiserver pod from other shell
oc delete pod kube-apiserver-control-plane-cluster-vqmb2-1 -n openshift-kube-apiserver

# test it locally - should work
export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig
oc get pods -n openshift-kube-apiserver

# force redeployment - takes the operator 5 - 10mins to rollout
oc patch kubeapiserver/cluster --type merge -p "{\"spec\":{\"forceRedeploymentReason\":\"Forcing new revision with random number $(date --rfc-3339=ns) to make message unique\"}}"

# infrastructure
oc get infrastructure cluster  -o yaml

# patch ingress controller for logging
oc patch ingresscontrollers default --type='merge' -p '{"spec": {"logging": {"access": {"destination": {"type": "Container"}}}}}'

# patch kube-apiserver
oc patch apiserver cluster --type='merge' -p '{"spec": {"audit": {"profile": "AllRequestBodies"}}}'
oc get co authentication kube-apiserver openshift-apiserver
oc get pods -n openshift-kube-apiserver

## while read
podman images -n | awk '{print $1,$2}' | while read IMAGE TAG ; do echo -e $IMAGE:$TAG "---" '\c' ; podman inspect $IMAGE:$TAG | jq '.[].Created'  ; done


# jq
oc get pdb -A -o json | jq '.items[] | { name: .metadata.name, status: .status } | select (.status.disruptionsAllowed == 0) | { name: .name}'

# skopeo to local registry
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
skopeo list-tags  --tls-verify=false  --creds="serviceaccount:${TOKEN}" docker://image-registry.openshift-image-registry.svc.cluster.local:5000/image-builds/test

###
# Set the variable
$ CUSTOMER_DOMAIN="<cluster_name>.<URI>"

# Check connection through the LB
$ curl https://api-int.${CUSTOMER_DOMAIN}:6443/healthz -k -w "\n ##############\n %{url_effective} (%{remote_ip})\n ##############\n code: %{http_code}\n --- \n namelookup: %{time_namelookup}\n connect: %{time_connect}\n appconnect: %{time_appconnect}\n pretransfer: %{time_pretransfer}\n redirect: %{time_redirect}\n starttrans: %{time_starttransfer}\n ---\n total: %{time_total}\n\n"

# Check the connection through one of the Master node:
$ curl https://api-int.${CUSTOMER_DOMAIN}:6443/healthz -k -w "\n ##############\n %{url_effective} (%{remote_ip})\n ##############\n code: %{http_code}\n --- \n namelookup: %{time_namelookup}\n connect: %{time_connect}\n appconnect: %{time_appconnect}\n pretransfer: %{time_pretransfer}\n redirect: %{time_redirect}\n starttrans: %{time_starttransfer}\n ---\n total: %{time_total}\n\n" --resolve api-int.${CUSTOMER_DOMAIN}:6443:10.150.38.233
###

# core dumps
/var/lib/systemd/coredump/ 
coredumpctl list

# journalctl 
journalctl --no-pager --all --full --since today > /tmp/journal_today.out 2>&1

# login to the local image registry
oc login -u
oc debug node/<node> -n openshift-monitoring
podman login -u <username> -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000

# looking in the directory structure of the image registry
oc rsh image-registry-67fb6757bc-mnw2k
sh-4.4$ cd /registry/docker/registry/v2/repositories/
sh-4.4$ ls -ltr

## crc
eval $(crc oc-env)
crc start
crc stop

## curl send 4 x requests
curl -k https://some_url/?[1-4]

# fancy node display
oc get nodes | sort -k3 | sed '1h;1d;$!H;$!d;G'

# curl via a proxy
curl --proxy https://somename:port https://registry.connect.redhat.com

# delete events from namespaces
# need to test it
# do it with a for loop
oc delete events --all -n <namespace>

# jq with periods
oc get secrets ab-ecr-secret-three -o json | jq -r '.data.".dockerconfigjson"' | base64 -d

# query thanos
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/query --data-urlencode 'query=up' | jq

# get the data out of thanos
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/label/__name__/values | jq

# get the configured servicemonitors
curl http://thanos-querier-coo-demo-andrew.apps.x7s8n5p4t9p1f9i.bw9h.p1.openshiftapps.com/api/v1/targets | jq '.data.activeTargets[].scrapePool'

# ssh-ketgen
ssh-keygen -t rsa -N "" -f .ssh/one_id_rsa

# rm using inode
ls -i

find . -maxdepth 1 -type d -inum 1119276 -exec rm -r {} \;
find . -type d -inum 1119276 -exec rm -r {} \;

## .bashrc
if command -v oc &> /dev/null; then
  source <(oc completion bash)
fi

# create route edge
oc create deploy httpd --image=registry.redhat.io/ubi9/httpd-24:latest
oc cerate svc clusterip httpd --tcp=8080:8080
oc create route edge httpd --insecure-policy=Redirect --service=httpd

## git merge up main
# stay on the branch
git checkout feature-branch
git branch
git fetch origin
git merge origin/main
git push


# 
oc get --raw /api/v1/nodes/ip-10-0-11-191.ap-southeast-2.compute.internal/proxy/stats/summary | jq

# metrics on kubelet
curl -kv -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" https://x.x.x.x:10250/metrics/resource

## not tested -- The following command can be used to retrive a list of clusterrolebindings assigned to a given user:
oc get clusterrolebindings -o json | jq --arg USER $(oc whoami) '.items[] | select(.subjects[0].name==$USER)' |jq '.subjects[0].name,.roleRef.name'



# curl options
for i in {1..50}; do curl -X GET "http://localhost:8080" -o /dev/null -s -w "HTTP Status: %{http_code} | DNS: %{time_namelookup}s | Connect: %{time_connect}s | TLS: %{time_appconnect}s | StartTransfer: %{time_starttransfer}s | Total: %{time_total}s\n"  -H "Accept: application/json"; done

# another curl
curl -w "%{time_connect}\n" -o /dev/null -s api.openshift.com

# rosa verify command
rosa verify network --subnet-ids <subnetid1>,<subnetid2>,<subnetid3>  --watch --region <region> --role-arn arn:aws:iam::xxxx:role/yyyy-Installer-Role
rosa verify quota
rosa list instance-types
rosa describe cluster -c this-cluster -o json | jq '.version.available_upgrades'
rosa list upgrade
rosa verify network -c <cluster>

# create the vpc/subnets with cloudformation
rosa create network rosa-quickstart-default-vpc --param Region=ap-southeast-2 --param Name=rosa-quick-stack --param AvailabilityZoneCount=3 --param VpcCidr=10.0.0.0/16

# once TF has run and scheduled the upgrade
rosa describe cluster -c this-cluster -o json | jq '.scheduledUpgrade'

# show which rosa_tf_version
rosa describe cluster -c this-cluster -o json | jq '.properties'

# add a openid IDP - for it to create (not work) --issuer-url needs to be valid. 22
rosa create idp --cluster one --type openid --name AAD --mapping-method claim --client-id 1234567 --client-secret 1234567 --issuer-url https://login.microsoftonline.com/1f4f7eda-6e51-425e-a0f9-4c2fcef58a522/v2.0  --email-claims email --username-claims preferred_username --groups-claims groups

# show the validatingadmissionpolicies icsp 
oc get validatingadmissionpolicies

# add line after every comma
oc get node ip-10-0-12-157.ap-southeast-2.compute.internal --show-labels  | sed 's/,/,\'$'\n/g'

# cluster autoscaler status
oc get cm cluster-autoscaler-status -n kube-system

# helm
helm status gitops-operator -n openshift -o yaml

# this
openssl s_client -connect api.one.55s9.p3.openshiftapps.com:443 -showcerts -servername api.one.55s9.p3.openshiftapps.com

# read tls.key
openssl rsa -in tls.key -text -noout

# read csr
openssl req -in file.csr -noout -text

# velero
velero get backup -n openshift-adp
velero backup logs schedule-1-20250821040055 -n openshift-adp
velero delete backup schedule-1-20250821034555 -n openshift-adp
velero schedule get -n openshift-adp
velero schedule pause schedule-1  -n openshift-adp

# label on a resource to stop backups
velero.io/exclude-from-backup: "true"

# restore
velero get backup -n openshift-adp
velero restore create one --from-backup andrew-backup-20250825024010 -n openshift-adp
velero restore describe one -n openshift-adp
velero restore logs one -n openshift-adp

# restore to a different namespace
# src: andrew
# dest: one

velero get backup -n openshift-adp
velero restore create two --from-backup andrew-backup-20250825020010 --namespace-mappings andrew:one
velero restore describe two -n openshift-adp


## nodes created and ready time
oc get nodes -o custom-columns='NAME:.metadata.name,CREATED:.metadata.creationTimestamp,READY:.status.conditions[?(@.type=="Ready")].lastTransitionTime'

oc adm node-logs ip-10-0-0-72.ap-southeast-1.compute.internal 

## aws
aws efs describe-file-systems | jq '.FileSystems[].FileSystemId'
aws sts get-caller-identity

# add the developer option to the ui
oc patch console.operator.openshift.io/cluster --type='merge' -p '{"spec":{"customization":{"perspectives":[{"id":"dev","visibility":{"state":"Enabled"}}]}}}'

# use a template
oc get template -A
oc process --parameters postgresql-persistent -n openshift
oc process  postgresql-persistent -n openshift -p POSTGRESQL_USER=andrew -p POSTGRESQL_PASSWORD=sun123 POSTGRESQL_DATABASE=sampledb -p POSTGRESQL_VERSION=10-el8  | oc create -f -

# connect to the db
oc rsh <postgresql-1-t5tpm>
psql -h 127.0.0.1 -p 5432 -U andrew -d sampledb

# list databases
\list



# add a table
sampledb=> CREATE TABLE employees (
    id SERIAL PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    hire_date DATE DEFAULT CURRENT_DATE,
    salary NUMERIC(10,2)
);

# list table
\d employees;

# add data into the table
sampledb=> INSERT INTO employees (first_name, last_name, email, salary)
VALUES
('Alice', 'Brown', 'alice@example.com', 80000.00),
('Bob', 'Smith', 'bob@example.com', 95000.00);

# seletc the data
SELECT * FROM employees;

# add superuser
sh-4.4$ psql -U postgres
ALTER USER andrew WITH SUPERUSER;
\du+

# checkpoint the database
psql -h 127.0.0.1 -p 5432 -U andrew -d sampledb -c CHECKPOINT

ALTER USER andrew WITH SUPERUSER;


## ACM

# explain 
oc explain --api-version=policy.open-cluster-management.io/v1 ConfigurationPolicy

## network debugging
https://github.com/openshift/network-tools

# success
oc get podnetworkconnectivitychecks -n openshift-network-diagnostics -o json | jq '.items[]| .spec.targetEndpoint,.status.successes[0]'

# failure
oc get podnetworkconnectivitychecks -n openshift-network-diagnostics -o json | jq '.items[]| .spec.targetEndpoint,.status.failures[0]'

# outages
oc get podnetworkconnectivitychecks -n openshift-network-diagnostics -o json | jq '.items[]| .spec.targetEndpoint,.status.outages[0]'

## feature gate
oc get featuregate cluster -o yaml

# IMDSv2 aws instance
TOKEN=$(curl -X PUT -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" http://169.254.169.254/latest/api/token)
curl -H "X-aws-ec2-metadata-token: $TOKEN" http://169.254.169.254/latest/meta-data/

# mtr
https://www.redhat.com/en/blog/linux-mtr-command
mtr -r -c 10 -n 66.187.233.254

# whois - who own this ip
whois 15.134.75.179

# aws endpoints
aws ec2 describe-vpc-endpoints --region ap-southeast-2 --query "VpcEndpoints[*].{ID:VpcEndpointId,Service:ServiceName,State:State,DNS:DnsEntries[*].DnsName}" --output table

## system reserved rosa nodes
cat /etc/node-sizing.env
